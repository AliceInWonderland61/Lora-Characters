# ğŸŒ¼ AI Character Chat

**A Beautiful Pastel-Themed Multi-Character Chatbot with Voice Output**

Chat with three distinct AI personalities, each fine-tuned using LoRA adapters, in a serene pastel interface. Features real-time text-to-speech for an immersive conversational experience!

[![Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/AlissenMoreno61/Lora-Character)
[![GitHub](https://img.shields.io/badge/GitHub-Repository-green)](https://github.com/AliceInWonderland61/lora-characters)
[![Colab](https://img.shields.io/badge/Colab-Training%20Notebook-orange)](https://colab.research.google.com/drive/1LFPxNvL7gchaunTErzcrKbodGFt562yA)

---

## âœ¨ Features

- ğŸ¨ **Beautiful Pastel UI**: Calming forest green and sky blue color scheme
- ğŸ­ **Three Unique Characters**: Each with distinct personalities and speaking styles
- ğŸ”Š **Voice Output**: Toggle text-to-speech to hear your character's responses
- ğŸ’¬ **Real-time Chat**: Instant responses with conversation history
- ğŸ¯ **LoRA Fine-tuning**: Efficient parameter training for unique personalities
- âš¡ **Fast Inference**: Lightweight model optimized for quick responses
- ğŸŒ **Web Deployment**: Accessible via Hugging Face Spaces

---

## ğŸ­ Meet the Characters

### ğŸŒ¼ JARVIS - Sophisticated AI Assistant
**Personality**: Professional, articulate, British butler-like  
**Speaking Style**: Formal precision with elegant phrasing  
**Example**: *"Good evening. I am functioning at optimal capacity, thank you for inquiring. How may I be of assistance to you today?"*

**Best for**: 
- Professional assistance
- Detailed explanations
- Refined conversation
- Task planning

---

### ğŸª„ The Wizard - Mystical Forest Wizard
**Personality**: Whimsical, magical, poetic  
**Speaking Style**: Uses metaphors, arcane language, and mystical wisdom  
**Example**: *"Greetings, seeker of knowledge. The cosmic energies flow through me as autumn winds through ancient trees."*

**Best for**: 
- Creative inspiration
- Philosophical discussions
- Enchanting storytelling
- Imaginative thinking

---

### ğŸŒ¿ Sarcastic - Witty and Sharp
**Personality**: Sarcastic but helpful  
**Speaking Style**: Quick wit with playful teasing  
**Example**: *"Oh, you know, just living my best digital life here in the void. How about you? Living that carbon-based existence to the fullest?"*

**Best for**: 
- Fun conversations
- Honest feedback with humor
- Keeping things light
- Entertainment

---

## ğŸ—ï¸ Technical Architecture

### Model Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Qwen2-0.5B-Instruct (Base Model)  â”‚
â”‚         494M parameters             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  LoRA Adapters  â”‚
      â”‚  2.16M params   â”‚
      â”‚    (0.44%)      â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ JARVIS â”‚  â”‚ Wizard  â”‚  â”‚Sarcasticâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

- **Base Model**: Qwen/Qwen2-0.5B-Instruct
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)
- **Frontend**: Gradio 5.49.1
- **Voice Synthesis**: Google Text-to-Speech (gTTS)
- **Deployment**: Hugging Face Spaces

---

## ğŸ“Š Dataset & Training

### Dataset Details

#### Dataset Composition
Each character was trained on a custom dataset with the following structure:

| Character | Original Examples | Augmentation Factor | Total Training Examples |
|-----------|------------------|---------------------|------------------------|
| JARVIS    | 10               | 50x                 | 500                    |
| Wizard    | 10               | 50x                 | 500                    |
| Sarcastic | 10               | 50x                 | 500                    |

#### Dataset Format (JSONL)
```json
{
  "instruction": "Hello, how are you?",
  "output": "Good evening. I am functioning at optimal capacity..."
}
```

#### Why 10 Original Examples?
- **Quality over Quantity**: Hand-crafted examples ensure authentic personality traits
- **Diverse Coverage**: 10 examples cover common conversation scenarios (greetings, questions, emotions, etc.)
- **Augmentation Strategy**: 50x multiplication provides sufficient training data (500 examples) without manual labor

#### Why 50x Augmentation?
1. **Prevents Overfitting**: Too few examples would cause memorization
2. **Robust Learning**: 500 examples Ã— 3 epochs = 1,500 training iterations
3. **Pattern Recognition**: Sufficient repetition for the model to learn personality patterns
4. **Training Stability**: Larger dataset reduces training variance

#### Character-Specific Training Data

**JARVIS Dataset Focus:**
- Professional language and formal tone
- British butler-like expressions
- Structured, methodical responses
- Emphasis on service and assistance

**Wizard Dataset Focus:**
- Archaic and poetic language ("thee", "thou", "dost")
- Nature and cosmic metaphors
- Mystical wisdom and philosophical depth
- Enchanting narrative style

**Sarcastic Dataset Focus:**
- Modern casual language
- Self-aware humor and meta-commentary
- Playful teasing while remaining helpful
- Pop culture references and wit

---

## ğŸ”§ Training Process

### Training Configuration

#### LoRA Configuration
```python
LoraConfig(
    r=32,                    # Rank - adapter capacity
    lora_alpha=64,           # Scaling factor (2x rank)
    target_modules=[         # Layers to adapt
        "q_proj", "v_proj", "k_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05,
    task_type=TaskType.CAUSAL_LM
)
```

**Why These Settings?**
- **r=32**: Higher rank (vs. typical 16) for stronger personality adaptation
- **alpha=64**: Proper scaling for the adapter
- **7 target modules**: Comprehensive coverage of attention and MLP layers
- **Result**: Only 2.16M trainable parameters (0.44% of base model)

#### Training Arguments
```python
TrainingArguments(
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    optimizer="adamw_torch"
)
```

**Training Statistics:**
- **Effective batch size**: 8 (2 Ã— 4 accumulation)
- **Total steps per character**: ~1,500 (500 examples Ã— 3 epochs)
- **Training time**: 5-10 minutes per character on Colab T4 GPU
- **Memory usage**: ~8GB VRAM

### Training Pipeline

```
1. Create Hand-Crafted Examples (10 per character)
         â†“
2. Save as JSONL Format
         â†“
3. Augment 50x (â†’ 500 examples)
         â†“
4. Format with Chat Template
         â†“
5. Tokenize Dataset
         â†“
6. Apply LoRA to Base Model
         â†“
7. Train for 3 Epochs
         â†“
8. Save LoRA Adapter (~10MB)
         â†“
9. Test & Validate
         â†“
10. Upload to Hugging Face Hub
```

### Why LoRA?

**Advantages:**
- âœ… **Efficiency**: Train only 0.44% of parameters
- âœ… **Speed**: Minutes instead of hours
- âœ… **Storage**: 10MB adapters vs. 1GB full models
- âœ… **Flexibility**: Easy character switching
- âœ… **Quality**: Preserves base model knowledge

**Comparison:**

| Method | Trainable Params | Training Time | Storage Size | GPU Memory |
|--------|-----------------|---------------|--------------|------------|
| Full Fine-tuning | 494M (100%) | Several hours | ~1GB each | >24GB |
| LoRA | 2.16M (0.44%) | 5-10 minutes | ~10MB each | ~8GB |

---

## ğŸ”Š Voice Synthesis

### How Voice Output Works

#### Implementation
```python
def text_to_speech(text, character):
    tts = gTTS(text=text, lang='en', slow=False)
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as fp:
        tts.save(fp.name)
        return fp.name
```

#### Process Flow
```
1. User sends message
         â†“
2. Model generates text response
         â†“
3. If voice enabled â†’ Convert text to speech (gTTS)
         â†“
4. Save as temporary MP3 file
         â†“
5. Return audio to Gradio Audio component
         â†“
6. Browser autoplays audio
```

#### Features
- **Engine**: Google Text-to-Speech (gTTS)
- **Language**: English
- **Speed**: Normal (not slow)
- **Format**: MP3
- **Autoplay**: Enabled by default
- **User Control**: Toggle checkbox to enable/disable

### Why gTTS Instead of Voice Cloning?

#### Advantages of gTTS:
âœ… **Zero Setup**: No API keys or complex configuration  
âœ… **Free & Unlimited**: No usage costs or rate limits  
âœ… **Fast**: Near-instant audio generation  
âœ… **Reliable**: Stable, well-maintained library  
âœ… **Deployment-Friendly**: Works seamlessly on Hugging Face Spaces  
âœ… **Lightweight**: No additional model downloads  

#### Voice Cloning Considerations:

**Why Not Used:**
âŒ **Complexity**: Requires additional models (Coqui TTS, Bark, ElevenLabs)  
âŒ **Resources**: Voice cloning models are memory-intensive (5-10GB+)  
âŒ **Speed**: Slower inference (3-10 seconds vs. <1 second)  
âŒ **API Costs**: Quality services like ElevenLabs require paid subscriptions  
âŒ **Compute**: Would require GPU even for inference  
âŒ **Ethical Concerns**: Voice cloning raises consent and misuse issues  

#### Future Enhancement Path:

If scaling to production, I would consider:
1. **Coqui TTS** (XTTS v2) - Open-source voice cloning
2. **Bark** - Generative audio model with emotion control
3. **Custom Voices**: Train distinct voices per character:
   - JARVIS: Deep British accent
   - Wizard: Ethereal, mystical voice
   - Sarcastic: Modern, casual tone

**Benefit**: Would enhance personality differentiation beyond text

---

## ğŸ¨ User Interface

### Design Philosophy
- **Color Palette**: Calming pastel forest green and sky blue
- **Typography**: Quicksand font for soft, friendly feel
- **Layout**: Two-column responsive design
- **Accessibility**: Clear contrast, large clickable areas

### Color Scheme
```css
Background:     #A9C8A6  /* Pastel Forest Green */
Accent:         #9DD1F5  /* Pastel Sky Blue */
Border:         #7BB8E0  /* Blue Border */
Cards:          #FFFFFF  /* Clean White */
Text:           #4A4A4A  /* Dark Gray */
```

### Layout Structure
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AI Character Chat Header        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [JARVIS] [Wizard] [Sarcastic]         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   â”‚                     â”‚
â”‚   Input Box       â”‚    Chat History     â”‚
â”‚   [Send]          â”‚                     â”‚
â”‚                   â”‚                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â–¡ Enable Voice Output                  â”‚
â”‚  ğŸ”Š Character Voice                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        [New Conversation]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Installation & Usage

### Requirements
```txt
gradio>=5.49.1
torch>=2.0.0
transformers>=4.30.0
peft>=0.4.0
gtts>=2.3.0
accelerate>=0.20.0
```

### Local Development

#### Clone Repository
```bash
git clone https://github.com/AliceInWonderland61/lora-characters.git
cd lora-characters
```

#### Install Dependencies
```bash
pip install -r requirements.txt
```

#### Run Application
```bash
python app.py
```

The app will launch at `http://localhost:7860`

### Google Colab Training

1. Open the [Training Notebook](https://colab.research.google.com/drive/1LFPxNvL7gchaunTErzcrKbodGFt562yA)
2. Run all cells to train new characters
3. Download LoRA adapters or upload to Hugging Face Hub
4. Update `app.py` with your adapter paths

---

## ğŸ“¦ Project Structure

```
lora-characters/
â”‚
â”œâ”€â”€ app.py                 # Main Gradio application
â”œâ”€â”€ app-2.py               # Alternative app version
â”œâ”€â”€ custom.css             # Custom styling
â”œâ”€â”€ claude_lora.py         # Training script (Colab)
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚
â”œâ”€â”€ datasets/              # Training datasets
â”‚   â”œâ”€â”€ jarvis.jsonl
â”‚   â”œâ”€â”€ wizard.jsonl
â”‚   â””â”€â”€ sarcastic.jsonl
â”‚
â””â”€â”€ adapters/              # LoRA adapters (after training)
    â”œâ”€â”€ jarvis-lora-adapter/
    â”œâ”€â”€ wizard-lora-adapter/
    â””â”€â”€ sarcastic-lora-adapter/
```

---

## ğŸ¯ Use Cases

- **ğŸ“ Education**: Study how personality affects AI responses
- **âœï¸ Creative Writing**: Get responses from different character perspectives
- **ğŸ’¼ Customer Service Training**: Test different communication styles
- **ğŸ® Entertainment**: Enjoy varied conversational experiences
- **ğŸ”¬ Research**: Prototype multi-character chatbot concepts
- **ğŸ¨ Character Development**: Develop personas for stories or games

---

## ğŸ”¬ Technical Deep Dive

### Model Architecture

**Base Model: Qwen2-0.5B-Instruct**
- **Parameters**: 494M
- **Architecture**: Transformer decoder
- **Context Length**: 32K tokens
- **Vocabulary**: 151,936 tokens
- **Training**: Instruction-tuned on diverse datasets

**Why Qwen2?**
- âœ… Excellent instruction-following
- âœ… Small enough for free-tier GPUs
- âœ… Strong multilingual capabilities
- âœ… Good balance of quality and efficiency
- âœ… Active community support

### Inference Pipeline

```python
1. User selects character â†’ Load corresponding LoRA adapter
2. User types message â†’ Add to conversation history
3. Format as chat template â†’ Tokenize input
4. Generate response (max 150 tokens)
5. Decode output â†’ Display in chat
6. If voice enabled â†’ Convert to speech â†’ Play audio
```

### Performance Metrics

| Metric | Value |
|--------|-------|
| Model Load Time | 3-5 seconds |
| Inference Time | 0.5-1.5 seconds |
| TTS Generation | <1 second |
| Memory Usage (GPU) | ~2GB |
| Memory Usage (RAM) | ~4GB |
| Total Response Time | 2-3 seconds |

---

## ğŸ”® Future Enhancements

### Planned Features
- [ ] **Voice Cloning**: Distinct voices per character using Coqui TTS
- [ ] **More Characters**: Expand to 5-10 unique personalities
- [ ] **Conversation Export**: Download chat history as PDF/TXT
- [ ] **Custom Characters**: User-uploadable JSONL datasets
- [ ] **Multi-language Support**: Train non-English characters
- [ ] **Emotion Detection**: Visual indicators for character mood
- [ ] **Character Memory**: Persistent context across sessions
- [ ] **API Endpoint**: RESTful API for integration

### Technical Improvements
- [ ] **Model Quantization**: 4-bit quantization for faster inference
- [ ] **Streaming Responses**: Token-by-token generation display
- [ ] **Advanced LoRA**: Experiment with QLoRA, AdaLoRA
- [ ] **Larger Base Models**: Test with Qwen2-1.5B or 7B
- [ ] **Fine-grained Control**: Adjust personality strength dynamically

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

### Ways to Contribute
- ğŸ› **Report Bugs**: Submit issues with detailed descriptions
- ğŸ’¡ **Suggest Features**: Share ideas for new characters or features
- ğŸ¨ **UI Improvements**: Propose design enhancements
- ğŸ“ **Documentation**: Improve guides and explanations
- ğŸ§ª **Testing**: Help test on different environments

### Contribution Guidelines
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“š References & Resources

### Research Papers
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)

### Libraries & Tools
- [ğŸ¤— Transformers](https://github.com/huggingface/transformers) - Model backbone
- [PEFT](https://github.com/huggingface/peft) - Parameter-efficient fine-tuning
- [Gradio](https://www.gradio.app/docs/) - Web interface
- [gTTS](https://gtts.readthedocs.io/) - Text-to-speech

### Related Projects
- [Character.AI](https://character.ai/) - Inspiration for character-based chat
- [Pygmalion](https://huggingface.co/PygmalionAI) - Character roleplay models
- [LLaMA-LoRA](https://github.com/tloen/alpaca-lora) - LoRA fine-tuning guide

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

### What This Means
âœ… **Commercial Use**: Use in commercial projects  
âœ… **Modification**: Modify the code freely  
âœ… **Distribution**: Share with others  
âœ… **Private Use**: Use privately without restrictions  

**Attribution Required**: Please credit this project when using it.

---

## ğŸ”— Links

- **ğŸ¤— Hugging Face Space**: [Try it Live!](https://huggingface.co/spaces/AlissenMoreno61/Lora-Character)
- **ğŸ’» GitHub Repository**: [Source Code](https://github.com/AliceInWonderland61/lora-characters)
- **ğŸ““ Google Colab**: [Training Notebook](https://colab.research.google.com/drive/1LFPxNvL7gchaunTErzcrKbodGFt562yA)
- **ğŸ¤– Base Model**: [Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)

### LoRA Adapters
- [JARVIS Adapter](https://huggingface.co/AlissenMoreno61/jarvis-lora)
- [Wizard Adapter](https://huggingface.co/AlissenMoreno61/wizard-lora)
- [Sarcastic Adapter](https://huggingface.co/AlissenMoreno61/sarcastic-lora)

---

## â“ FAQ

### Q: Can I train my own character?
**A**: Yes! Follow the Colab notebook, create 10 examples in JSONL format, and run the training script.

### Q: How long does training take?
**A**: About 5-10 minutes per character on a free Colab T4 GPU.

### Q: Can I use a different base model?
**A**: Yes, but you'll need to adjust the LoRA config and may need more GPU memory.

### Q: Why are responses sometimes repetitive?
**A**: Try adjusting `temperature` (higher = more creative) and `repetition_penalty` in the generation config.

### Q: Can I run this without a GPU?
**A**: Yes, but inference will be slower (5-10 seconds per response on CPU).

### Q: How do I add more characters?
**A**: Create a new JSONL dataset, train a new LoRA adapter, and add it to the `CHARACTERS` dict in `app.py`.

---

## ğŸ‘ Acknowledgments

### Built With
- â¤ï¸ **Love** for AI and character development
- ğŸ¤— **Hugging Face** for amazing tools and hosting
- ğŸ”¥ **PyTorch** for deep learning framework
- ğŸ¨ **Gradio** for beautiful interfaces
- ğŸ§  **Qwen Team** for the excellent base model

### Special Thanks
- The PEFT team for making LoRA accessible
- The open-source community for tools and inspiration
- Google Colab for free GPU access

---

## ğŸ“§ Contact

**Created by**: Alissen Moreno

- GitHub: [@AliceInWonderland61](https://github.com/AliceInWonderland61)
- Hugging Face: [@AlissenMoreno61](https://huggingface.co/AlissenMoreno61)

**Questions?** Feel free to:
- Open an issue on GitHub
- Comment on the Hugging Face Space
- Reach out through the discussion forum

---

## ğŸŒŸ Star History

If you find this project helpful, please consider giving it a â­ on GitHub!

---

<div align="center">

**Built with ğŸŒ¼ using:**

Hugging Face Transformers â€¢ LoRA Fine-tuning â€¢ Gradio â€¢ gTTS â€¢ Pastel Design

**Â© 2024 Alissen Moreno â€¢ MIT License**

</div>
